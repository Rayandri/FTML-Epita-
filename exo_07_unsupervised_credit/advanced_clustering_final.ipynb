{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ðŸš€ Advanced Credit Card Customer Clustering\n",
        "\n",
        "**FTML 2025 - Exercice 7 : Apprentissage Non-SupervisÃ© OptimisÃ©**\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŽ¯ **Objectifs**\n",
        "Segmentation avancÃ©e des clients de carte de crÃ©dit utilisant les **techniques state-of-the-art** et exploitant **32 cÅ“urs CPU + RTX 5060 GPU**.\n",
        "\n",
        "## ðŸ“Š **Dataset**\n",
        "- **Source** : UCI Credit Card Default - Taiwan\n",
        "- **Taille** : 30,000 clients Ã— 24 variables\n",
        "- **Optimisations** : GPU RAPIDS cuML, UMAP, clustering parallÃ¨le\n",
        "\n",
        "## ðŸ”¥ **Techniques Modernes IntÃ©grÃ©es**\n",
        "1. **GPU Acceleration** : cuML RAPIDS (15x-312x speedup)\n",
        "2. **UMAP** : RÃ©duction dimensionnelle supÃ©rieure Ã  PCA\n",
        "3. **Feature Engineering AvancÃ©** : 50+ variables dÃ©rivÃ©es\n",
        "4. **Clustering Multiple** : K-means, Hierarchical, GMM, DBSCAN\n",
        "5. **Visualisations Pro** : Yellowbrick, dendrogrammes interactifs\n",
        "6. **Analyse Business** : Profils clients dÃ©taillÃ©s\n",
        "7. **ParallÃ©lisation** : Utilisation des 32 cÅ“urs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from joblib import Parallel, delayed\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from scipy.stats import chi2_contingency\n",
        "import time\n",
        "from collections import Counter\n",
        "\n",
        "# GPU Libraries (with fallback)\n",
        "try:\n",
        "    import cuml\n",
        "    import cudf\n",
        "    import cupy as cp\n",
        "    from cuml.cluster import KMeans as cuKMeans, DBSCAN as cuDBSCAN\n",
        "    from cuml.manifold import UMAP as cuUMAP\n",
        "    from cuml.decomposition import PCA as cuPCA\n",
        "    GPU_AVAILABLE = True\n",
        "    print(\"ðŸš€ GPU cuML RAPIDS detected - Using GPU acceleration!\")\n",
        "except ImportError:\n",
        "    GPU_AVAILABLE = False\n",
        "    print(\"âš ï¸  cuML not available - Using CPU fallback\")\n",
        "\n",
        "# Advanced visualization libraries\n",
        "try:\n",
        "    import umap\n",
        "    UMAP_AVAILABLE = True\n",
        "    print(\"âœ… UMAP available\")\n",
        "except ImportError:\n",
        "    UMAP_AVAILABLE = False\n",
        "    print(\"âš ï¸  UMAP not available - Using PCA fallback\")\n",
        "\n",
        "try:\n",
        "    from yellowbrick.cluster import SilhouetteVisualizer, KElbowVisualizer\n",
        "    from yellowbrick.features import ParallelCoordinates\n",
        "    YELLOWBRICK_AVAILABLE = True\n",
        "    print(\"âœ… Yellowbrick available for advanced visualizations\")\n",
        "except ImportError:\n",
        "    YELLOWBRICK_AVAILABLE = False\n",
        "    print(\"âš ï¸  Yellowbrick not available - Using standard visualizations\")\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(f\"ðŸ”§ Configuration: GPU={'âœ…' if GPU_AVAILABLE else 'âŒ'} | UMAP={'âœ…' if UMAP_AVAILABLE else 'âŒ'} | Yellowbrick={'âœ…' if YELLOWBRICK_AVAILABLE else 'âŒ'}\")\n",
        "print(f\"ðŸ’» Ready to use 32 CPU cores + RTX 5060 GPU!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. ðŸ“¥ Chargement et Exploration des DonnÃ©es\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_and_explore_data():\n",
        "    \"\"\"Chargement et exploration initiale du dataset\"\"\"\n",
        "    # Chargement des donnÃ©es\n",
        "    df = pd.read_csv('../data/default_of_credit_card_clients.csv')\n",
        "    \n",
        "    print(f\"ðŸ“Š Dataset chargÃ© : {df.shape[0]:,} observations Ã— {df.shape[1]} variables\")\n",
        "    print(f\"ðŸ’¾ Taille mÃ©moire : {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
        "    \n",
        "    # Informations de base\n",
        "    print(f\"\\nðŸ” Structure du dataset :\")\n",
        "    print(f\"   â€¢ Variables numÃ©riques : {df.select_dtypes(include=[np.number]).shape[1]}\")\n",
        "    print(f\"   â€¢ Variables catÃ©gorielles : {df.select_dtypes(include=['object']).shape[1]}\")\n",
        "    print(f\"   â€¢ Valeurs manquantes : {df.isnull().sum().sum()}\")\n",
        "    \n",
        "    # Distribution de la variable cible (pour validation finale)\n",
        "    if 'default payment next month' in df.columns:\n",
        "        default_rate = df['default payment next month'].mean()\n",
        "        print(f\"   â€¢ Taux de dÃ©faut : {default_rate:.1%}\")\n",
        "    \n",
        "    # Statistiques descriptives clÃ©s\n",
        "    print(f\"\\nðŸ’³ CaractÃ©ristiques clÃ©s :\")\n",
        "    print(f\"   â€¢ Ã‚ge moyen : {df['AGE'].mean():.1f} ans (Ã©cart-type : {df['AGE'].std():.1f})\")\n",
        "    print(f\"   â€¢ Limite de crÃ©dit mÃ©diane : {df['LIMIT_BAL'].median():,.0f} NT$\")\n",
        "    print(f\"   â€¢ Utilisation moyenne : {(df['BILL_AMT1'] / df['LIMIT_BAL']).mean():.1%}\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Chargement\n",
        "df = load_and_explore_data()\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. ðŸ”§ Feature Engineering AvancÃ©\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def advanced_feature_engineering(df):\n",
        "    \"\"\"\n",
        "    Feature engineering avancÃ© basÃ© sur les meilleures pratiques\n",
        "    Inspiration: GitHub repos populaires + articles de recherche\n",
        "    \"\"\"\n",
        "    print(\"ðŸ”§ DÃ©but du feature engineering avancÃ©...\")\n",
        "    df_fe = df.copy()\n",
        "    \n",
        "    # Suppression des colonnes non pertinentes pour le clustering\n",
        "    columns_to_drop = ['ID']\n",
        "    if 'default payment next month' in df_fe.columns:\n",
        "        df_fe['target'] = df_fe['default payment next month']  # Sauvegarde pour validation\n",
        "        columns_to_drop.append('default payment next month')\n",
        "    \n",
        "    df_fe = df_fe.drop(columns=columns_to_drop, errors='ignore')\n",
        "    \n",
        "    # === 1. RATIOS FINANCIERS DE BASE ===\n",
        "    print(\"   ðŸ’° CrÃ©ation des ratios financiers de base...\")\n",
        "    \n",
        "    # Utilisation du crÃ©dit\n",
        "    df_fe['credit_utilization'] = df_fe['BILL_AMT1'] / (df_fe['LIMIT_BAL'] + 1)\n",
        "    df_fe['max_utilization'] = df_fe[['BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', \n",
        "                                     'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']].max(axis=1) / (df_fe['LIMIT_BAL'] + 1)\n",
        "    \n",
        "    # Ratios de paiement\n",
        "    df_fe['payment_ratio'] = df_fe['PAY_AMT1'] / (df_fe['BILL_AMT1'] + 1)\n",
        "    df_fe['payment_to_limit'] = df_fe['PAY_AMT1'] / (df_fe['LIMIT_BAL'] + 1)\n",
        "    \n",
        "    # === 2. AGRÃ‰GATIONS TEMPORELLES ===\n",
        "    print(\"   ðŸ“Š AgrÃ©gations temporelles (6 mois)...\")\n",
        "    \n",
        "    bill_cols = ['BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']\n",
        "    pay_cols = ['PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n",
        "    delay_cols = ['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']\n",
        "    \n",
        "    # Moyennes et mÃ©dianes\n",
        "    df_fe['avg_bill'] = df_fe[bill_cols].mean(axis=1)\n",
        "    df_fe['median_bill'] = df_fe[bill_cols].median(axis=1)\n",
        "    df_fe['avg_payment'] = df_fe[pay_cols].mean(axis=1)\n",
        "    df_fe['median_payment'] = df_fe[pay_cols].median(axis=1)\n",
        "    \n",
        "    # Statistiques de retard\n",
        "    df_fe['avg_delay'] = df_fe[delay_cols].mean(axis=1)\n",
        "    df_fe['max_delay'] = df_fe[delay_cols].max(axis=1)\n",
        "    df_fe['delay_count'] = (df_fe[delay_cols] > 0).sum(axis=1)\n",
        "    \n",
        "    # === 3. VOLATILITÃ‰ ET STABILITÃ‰ ===\n",
        "    print(\"   ðŸ“ˆ Calcul de la volatilitÃ© et stabilitÃ©...\")\n",
        "    \n",
        "    # VolatilitÃ© des montants\n",
        "    df_fe['bill_volatility'] = df_fe[bill_cols].std(axis=1) / (df_fe[bill_cols].mean(axis=1) + 1)\n",
        "    df_fe['payment_volatility'] = df_fe[pay_cols].std(axis=1) / (df_fe[pay_cols].mean(axis=1) + 1)\n",
        "    df_fe['delay_volatility'] = df_fe[delay_cols].std(axis=1)\n",
        "    \n",
        "    # Consistance des paiements\n",
        "    df_fe['payment_consistency'] = 1 - (df_fe[pay_cols].std(axis=1) / (df_fe[pay_cols].mean(axis=1) + 1))\n",
        "    df_fe['payment_frequency'] = (df_fe[pay_cols] > 0).sum(axis=1) / 6\n",
        "    \n",
        "    # === 4. TENDANCES TEMPORELLES ===\n",
        "    print(\"   ðŸ“‰ Analyse des tendances temporelles...\")\n",
        "    \n",
        "    # Tendances (rÃ©cent vs ancien)\n",
        "    recent_bills = df_fe[bill_cols[:3]].mean(axis=1)\n",
        "    older_bills = df_fe[bill_cols[3:]].mean(axis=1)\n",
        "    df_fe['bill_trend'] = (recent_bills - older_bills) / (df_fe['avg_bill'] + 1)\n",
        "    \n",
        "    recent_payments = df_fe[pay_cols[:3]].mean(axis=1)\n",
        "    older_payments = df_fe[pay_cols[3:]].mean(axis=1)\n",
        "    df_fe['payment_trend'] = (recent_payments - older_payments) / (df_fe['avg_payment'] + 1)\n",
        "    \n",
        "    recent_delays = df_fe[delay_cols[:3]].mean(axis=1)\n",
        "    older_delays = df_fe[delay_cols[3:]].mean(axis=1)\n",
        "    df_fe['delay_trend'] = recent_delays - older_delays\n",
        "    \n",
        "    # === 5. RATIOS AVANCÃ‰S ===\n",
        "    print(\"   ðŸŽ¯ Ratios avancÃ©s et interactions...\")\n",
        "    \n",
        "    # Ratios comportementaux\n",
        "    df_fe['payment_bill_ratio'] = df_fe['avg_payment'] / (df_fe['avg_bill'] + 1)\n",
        "    df_fe['limit_age_ratio'] = df_fe['LIMIT_BAL'] / (df_fe['AGE'] + 1)\n",
        "    df_fe['bill_limit_ratio'] = df_fe['avg_bill'] / (df_fe['LIMIT_BAL'] + 1)\n",
        "    \n",
        "    # Variables d'interaction\n",
        "    df_fe['age_limit_interaction'] = df_fe['AGE'] * df_fe['LIMIT_BAL'] / 1000\n",
        "    df_fe['education_limit_interaction'] = df_fe['EDUCATION'] * df_fe['LIMIT_BAL'] / 1000\n",
        "    df_fe['age_utilization'] = df_fe['AGE'] * df_fe['credit_utilization']\n",
        "    \n",
        "    # === 6. INDICATEURS DE RISQUE ===\n",
        "    print(\"   âš ï¸  CrÃ©ation d'indicateurs de risque...\")\n",
        "    \n",
        "    # Seuils basÃ©s sur l'analyse exploratoire\n",
        "    df_fe['high_utilization'] = (df_fe['credit_utilization'] > 0.8).astype(int)\n",
        "    df_fe['frequent_delays'] = (df_fe['delay_count'] >= 3).astype(int)\n",
        "    df_fe['low_payment_ratio'] = (df_fe['payment_ratio'] < 0.1).astype(int)\n",
        "    df_fe['high_volatility'] = (df_fe['bill_volatility'] > 1.0).astype(int)\n",
        "    df_fe['payment_issues'] = (df_fe['payment_frequency'] < 0.5).astype(int)\n",
        "    \n",
        "    # Score de risque composite\n",
        "    risk_indicators = ['high_utilization', 'frequent_delays', 'low_payment_ratio', \n",
        "                      'high_volatility', 'payment_issues']\n",
        "    df_fe['risk_score'] = df_fe[risk_indicators].sum(axis=1)\n",
        "    \n",
        "    # === 7. PROFILS FINANCIERS ===\n",
        "    print(\"   ðŸ‘¤ CrÃ©ation de profils financiers...\")\n",
        "    \n",
        "    # Segmentation par utilisation\n",
        "    def categorize_utilization(x):\n",
        "        if x < 0.3: return 'Low'\n",
        "        elif x < 0.7: return 'Medium'\n",
        "        else: return 'High'\n",
        "    \n",
        "    df_fe['utilization_category'] = df_fe['credit_utilization'].apply(categorize_utilization)\n",
        "    \n",
        "    # Segmentation par limite\n",
        "    df_fe['limit_category'] = pd.qcut(df_fe['LIMIT_BAL'], q=4, labels=['Low', 'Medium', 'High', 'Premium'])\n",
        "    \n",
        "    # === 8. NETTOYAGE FINAL ===\n",
        "    print(\"   ðŸ§¹ Nettoyage et finalisation...\")\n",
        "    \n",
        "    # Gestion des valeurs infinies et NaN\n",
        "    df_fe = df_fe.replace([np.inf, -np.inf], np.nan)\n",
        "    df_fe = df_fe.fillna(0)\n",
        "    \n",
        "    # Variables catÃ©gorielles en numÃ©rique pour le clustering\n",
        "    if 'utilization_category' in df_fe.columns:\n",
        "        df_fe['utilization_category'] = pd.Categorical(df_fe['utilization_category'], \n",
        "                                                       categories=['Low', 'Medium', 'High']).codes\n",
        "    if 'limit_category' in df_fe.columns:\n",
        "        df_fe['limit_category'] = pd.Categorical(df_fe['limit_category'], \n",
        "                                                 categories=['Low', 'Medium', 'High', 'Premium']).codes\n",
        "    \n",
        "    print(f\"âœ… Feature engineering terminÃ© : {df_fe.shape[1]} variables crÃ©Ã©es\")\n",
        "    print(f\"   â€¢ Variables originales : 23\")\n",
        "    print(f\"   â€¢ Nouvelles variables : {df_fe.shape[1] - 23}\")\n",
        "    \n",
        "    return df_fe\n",
        "\n",
        "# Application du feature engineering\n",
        "df_enhanced = advanced_feature_engineering(df)\n",
        "print(f\"\\nðŸ“Š Dataset enrichi : {df_enhanced.shape}\")\n",
        "df_enhanced.head()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. ðŸŽ›ï¸ Preprocessing Intelligent et RÃ©duction Dimensionnelle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
