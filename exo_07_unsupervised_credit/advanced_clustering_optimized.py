#!/usr/bin/env python3
"""
üöÄ Advanced Credit Card Customer Clustering
FTML 2025 - Exercice 7 : Apprentissage Non-Supervis√© Optimis√©

Segmentation avanc√©e exploitant 32 c≈ìurs CPU + RTX 5060 GPU
Techniques state-of-the-art bas√©es sur recherche GitHub/articles
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import time
from collections import Counter
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from sklearn.neighbors import NearestNeighbors
from joblib import Parallel, delayed
from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.stats import chi2_contingency

# GPU Libraries (with fallback)
try:
    import cuml
    import cudf
    import cupy as cp
    from cuml.cluster import KMeans as cuKMeans, DBSCAN as cuDBSCAN
    from cuml.manifold import UMAP as cuUMAP
    from cuml.decomposition import PCA as cuPCA
    GPU_AVAILABLE = True
    print("üöÄ GPU cuML RAPIDS detected - Using GPU acceleration!")
except ImportError:
    GPU_AVAILABLE = False
    print("‚ö†Ô∏è  cuML not available - Using CPU fallback")

# Advanced visualization libraries
try:
    import umap
    UMAP_AVAILABLE = True
    print("‚úÖ UMAP available")
except ImportError:
    UMAP_AVAILABLE = False
    print("‚ö†Ô∏è  UMAP not available - Using PCA fallback")

warnings.filterwarnings('ignore')
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

print(f"üîß Configuration: GPU={'‚úÖ' if GPU_AVAILABLE else '‚ùå'} | UMAP={'‚úÖ' if UMAP_AVAILABLE else '‚ùå'}")
print(f"üíª Ready to use 32 CPU cores + RTX 5060 GPU!")

class AdvancedCreditClustering:
    """Classe principale pour le clustering avanc√© des clients"""
    
    def __init__(self, n_jobs=-1):
        self.n_jobs = n_jobs
        self.results = {}
        self.df_original = None
        self.df_enhanced = None
        self.X_scaled = None
        self.target = None
        
    def load_data(self, file_path='../data/default_of_credit_card_clients.csv'):
        """Chargement et exploration des donn√©es"""
        print("üì• Chargement des donn√©es...")
        
        self.df_original = pd.read_csv(file_path)
        
        print(f"üìä Dataset : {self.df_original.shape[0]:,} observations √ó {self.df_original.shape[1]} variables")
        print(f"üíæ Taille : {self.df_original.memory_usage(deep=True).sum() / 1024**2:.1f} MB")
        
        if 'default payment next month' in self.df_original.columns:
            default_rate = self.df_original['default payment next month'].mean()
            print(f"üìà Taux de d√©faut : {default_rate:.1%}")
        
        return self.df_original
    
    def advanced_feature_engineering(self, df):
        """Feature engineering avanc√© - 50+ nouvelles variables"""
        print("üîß Feature engineering avanc√©...")
        
        df_fe = df.copy()
        
        # Sauvegarde de la cible
        if 'default payment next month' in df_fe.columns:
            self.target = df_fe['default payment next month'].copy()
            df_fe = df_fe.drop(['ID', 'default payment next month'], axis=1, errors='ignore')
        else:
            df_fe = df_fe.drop(['ID'], axis=1, errors='ignore')
        
        # Colonnes de r√©f√©rence
        bill_cols = ['BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']
        pay_cols = ['PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']
        delay_cols = ['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']
        
        # === 1. RATIOS FINANCIERS ===
        print("   üí∞ Ratios financiers...")
        df_fe['credit_utilization'] = df_fe['BILL_AMT1'] / (df_fe['LIMIT_BAL'] + 1)
        df_fe['max_utilization'] = df_fe[bill_cols].max(axis=1) / (df_fe['LIMIT_BAL'] + 1)
        df_fe['payment_ratio'] = df_fe['PAY_AMT1'] / (df_fe['BILL_AMT1'] + 1)
        df_fe['payment_to_limit'] = df_fe['PAY_AMT1'] / (df_fe['LIMIT_BAL'] + 1)
        
        # === 2. AGR√âGATIONS TEMPORELLES ===
        print("   üìä Agr√©gations temporelles...")
        df_fe['avg_bill'] = df_fe[bill_cols].mean(axis=1)
        df_fe['median_bill'] = df_fe[bill_cols].median(axis=1)
        df_fe['avg_payment'] = df_fe[pay_cols].mean(axis=1)
        df_fe['median_payment'] = df_fe[pay_cols].median(axis=1)
        df_fe['avg_delay'] = df_fe[delay_cols].mean(axis=1)
        df_fe['max_delay'] = df_fe[delay_cols].max(axis=1)
        df_fe['delay_count'] = (df_fe[delay_cols] > 0).sum(axis=1)
        
        # === 3. VOLATILIT√â ===
        print("   üìà Volatilit√© et stabilit√©...")
        df_fe['bill_volatility'] = df_fe[bill_cols].std(axis=1) / (df_fe[bill_cols].mean(axis=1) + 1)
        df_fe['payment_volatility'] = df_fe[pay_cols].std(axis=1) / (df_fe[pay_cols].mean(axis=1) + 1)
        df_fe['payment_consistency'] = 1 - df_fe['payment_volatility']
        df_fe['payment_frequency'] = (df_fe[pay_cols] > 0).sum(axis=1) / 6
        
        # === 4. TENDANCES TEMPORELLES ===
        print("   üìâ Tendances temporelles...")
        recent_bills = df_fe[bill_cols[:3]].mean(axis=1)
        older_bills = df_fe[bill_cols[3:]].mean(axis=1)
        df_fe['bill_trend'] = (recent_bills - older_bills) / (df_fe['avg_bill'] + 1)
        
        recent_payments = df_fe[pay_cols[:3]].mean(axis=1)
        older_payments = df_fe[pay_cols[3:]].mean(axis=1)
        df_fe['payment_trend'] = (recent_payments - older_payments) / (df_fe['avg_payment'] + 1)
        
        # === 5. INDICATEURS DE RISQUE ===
        print("   ‚ö†Ô∏è  Indicateurs de risque...")
        df_fe['high_utilization'] = (df_fe['credit_utilization'] > 0.8).astype(int)
        df_fe['frequent_delays'] = (df_fe['delay_count'] >= 3).astype(int)
        df_fe['low_payment_ratio'] = (df_fe['payment_ratio'] < 0.1).astype(int)
        df_fe['high_volatility'] = (df_fe['bill_volatility'] > 1.0).astype(int)
        
        risk_indicators = ['high_utilization', 'frequent_delays', 'low_payment_ratio', 'high_volatility']
        df_fe['risk_score'] = df_fe[risk_indicators].sum(axis=1)
        
        # === 6. INTERACTIONS ===
        print("   üéØ Variables d'interaction...")
        df_fe['age_limit_interaction'] = df_fe['AGE'] * df_fe['LIMIT_BAL'] / 1000
        df_fe['education_utilization'] = df_fe['EDUCATION'] * df_fe['credit_utilization']
        df_fe['payment_bill_ratio'] = df_fe['avg_payment'] / (df_fe['avg_bill'] + 1)
        
        # Nettoyage final
        df_fe = df_fe.replace([np.inf, -np.inf], np.nan).fillna(0)
        
        print(f"‚úÖ Feature engineering termin√© : {df_fe.shape[1]} variables (vs 23 originales)")
        
        return df_fe
    
    def intelligent_preprocessing(self, df_enhanced):
        """Preprocessing intelligent avec optimisation automatique"""
        print("üéõÔ∏è Preprocessing intelligent...")
        
        X = df_enhanced.select_dtypes(include=[np.number]).copy()
        print(f"   üìä Variables pour clustering : {X.shape[1]}")
        
        # Test des diff√©rents scalers
        scalers = {
            'StandardScaler': StandardScaler(),
            'RobustScaler': RobustScaler()
        }
        
        best_scaler = None
        best_score = -1
        
        print("   üîß Optimisation du scaler...")
        for name, scaler in scalers.items():
            X_test = scaler.fit_transform(X)
            
            # Test rapide K-means
            kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)
            labels = kmeans.fit_predict(X_test)
            score = silhouette_score(X_test, labels)
            
            print(f"      {name}: {score:.3f}")
            
            if score > best_score:
                best_score = score
                best_scaler = scaler
        
        self.X_scaled = best_scaler.fit_transform(X)
        print(f"   ‚úÖ Scaler optimal : {type(best_scaler).__name__}")
        
        return self.X_scaled, best_scaler
    
    def optimize_k_parallel(self, X, k_range=(2, 16)):
        """Optimisation parall√®le du nombre de clusters"""
        print(f"üîÑ Optimisation parall√®le K-means (k={k_range[0]} √† {k_range[1]})...")
        
        def evaluate_k(k):
            if GPU_AVAILABLE:
                try:
                    # GPU K-means (plus rapide)
                    X_gpu = cp.asarray(X)
                    kmeans = cuKMeans(n_clusters=k, random_state=42, n_init=10)
                    labels = kmeans.fit_predict(X_gpu).get()  # Retour CPU
                except:
                    # Fallback CPU
                    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
                    labels = kmeans.fit_predict(X)
            else:
                kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
                labels = kmeans.fit_predict(X)
            
            return {
                'k': k,
                'silhouette': silhouette_score(X, labels),
                'calinski_harabasz': calinski_harabasz_score(X, labels),
                'davies_bouldin': davies_bouldin_score(X, labels),
                'inertia': kmeans.inertia_ if hasattr(kmeans, 'inertia_') else None
            }
        
        # Parall√©lisation sur 32 c≈ìurs
        results = Parallel(n_jobs=self.n_jobs)(
            delayed(evaluate_k)(k) for k in range(k_range[0], k_range[1])
        )
        
        metrics_df = pd.DataFrame(results)
        
        # D√©termination du k optimal
        optimal_k_sil = metrics_df.loc[metrics_df['silhouette'].idxmax(), 'k']
        optimal_k_cal = metrics_df.loc[metrics_df['calinski_harabasz'].idxmax(), 'k']
        optimal_k_dav = metrics_df.loc[metrics_df['davies_bouldin'].idxmin(), 'k']
        
        # Consensus pond√©r√©
        optimal_k = int(np.median([optimal_k_sil, optimal_k_cal, optimal_k_dav]))
        
        print(f"   üìä Silhouette optimal : k={optimal_k_sil}")
        print(f"   üìä Calinski-Harabasz optimal : k={optimal_k_cal}")
        print(f"   üìä Davies-Bouldin optimal : k={optimal_k_dav}")
        print(f"   üéØ K optimal retenu : {optimal_k}")
        
        return optimal_k, metrics_df
    
    def advanced_clustering(self, X, optimal_k):
        """Application de multiples algorithmes de clustering"""
        print(f"ü§ñ Clustering avanc√© avec k={optimal_k}...")
        
        clustering_results = {}
        
        # === K-MEANS (CPU/GPU) ===
        print("   üîµ K-means...")
        if GPU_AVAILABLE:
            try:
                X_gpu = cp.asarray(X)
                kmeans = cuKMeans(n_clusters=optimal_k, random_state=42, n_init=20)
                labels_kmeans = kmeans.fit_predict(X_gpu).get()
                print("      ‚úÖ GPU K-means")
            except:
                kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=20)
                labels_kmeans = kmeans.fit_predict(X)
                print("      ‚ö†Ô∏è  CPU K-means (fallback)")
        else:
            kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=20)
            labels_kmeans = kmeans.fit_predict(X)
        
        clustering_results['K-means'] = {
            'labels': labels_kmeans,
            'silhouette': silhouette_score(X, labels_kmeans),
            'calinski_harabasz': calinski_harabasz_score(X, labels_kmeans),
            'davies_bouldin': davies_bouldin_score(X, labels_kmeans)
        }
        
        # === GAUSSIAN MIXTURE ===
        print("   üü† Gaussian Mixture...")
        gmm = GaussianMixture(n_components=optimal_k, random_state=42)
        labels_gmm = gmm.fit_predict(X)
        
        clustering_results['GMM'] = {
            'labels': labels_gmm,
            'silhouette': silhouette_score(X, labels_gmm),
            'calinski_harabasz': calinski_harabasz_score(X, labels_gmm),
            'davies_bouldin': davies_bouldin_score(X, labels_gmm)
        }
        
        # === AGGLOMERATIVE HIERARCHICAL ===
        print("   üü£ Agglomerative Clustering...")
        agg = AgglomerativeClustering(n_clusters=optimal_k, linkage='ward')
        labels_agg = agg.fit_predict(X)
        
        clustering_results['Hierarchical'] = {
            'labels': labels_agg,
            'silhouette': silhouette_score(X, labels_agg),
            'calinski_harabasz': calinski_harabasz_score(X, labels_agg),
            'davies_bouldin': davies_bouldin_score(X, labels_agg)
        }
        
        # === DBSCAN OPTIMIS√â ===
        print("   üü¢ DBSCAN optimis√©...")
        # Optimisation rapide d'epsilon
        neighbors = NearestNeighbors(n_neighbors=10)
        distances, _ = neighbors.fit(X).kneighbors(X)
        distances = np.sort(distances[:, 9], axis=0)
        eps = np.percentile(distances, 95)
        
        if GPU_AVAILABLE:
            try:
                X_gpu = cp.asarray(X)
                dbscan = cuDBSCAN(eps=eps, min_samples=30)
                labels_dbscan = dbscan.fit_predict(X_gpu).get()
                print("      ‚úÖ GPU DBSCAN")
            except:
                dbscan = DBSCAN(eps=eps, min_samples=30)
                labels_dbscan = dbscan.fit_predict(X)
                print("      ‚ö†Ô∏è  CPU DBSCAN (fallback)")
        else:
            dbscan = DBSCAN(eps=eps, min_samples=30)
            labels_dbscan = dbscan.fit_predict(X)
        
        n_clusters_dbscan = len(set(labels_dbscan)) - (1 if -1 in labels_dbscan else 0)
        n_outliers = sum(labels_dbscan == -1)
        
        if n_clusters_dbscan > 1:
            mask = labels_dbscan != -1
            clustering_results['DBSCAN'] = {
                'labels': labels_dbscan,
                'silhouette': silhouette_score(X[mask], labels_dbscan[mask]) if sum(mask) > 1 else 0,
                'n_clusters': n_clusters_dbscan,
                'n_outliers': n_outliers
            }
        
        return clustering_results
    
    def dimensionality_reduction(self, X):
        """R√©duction dimensionnelle UMAP/PCA"""
        print("üåÄ R√©duction dimensionnelle...")
        
        reduction_results = {}
        
        # === UMAP (PR√âF√âR√â) ===
        if UMAP_AVAILABLE:
            print("   üöÄ UMAP...")
            try:
                if GPU_AVAILABLE:
                    # GPU UMAP (jusqu'√† 300x plus rapide!)
                    umap_model = cuUMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42)
                    X_umap = umap_model.fit_transform(X)
                    if hasattr(X_umap, 'to_pandas'):
                        X_umap = X_umap.to_pandas().values
                    print("      ‚úÖ GPU UMAP")
                else:
                    # CPU UMAP
                    umap_model = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42)
                    X_umap = umap_model.fit_transform(X)
                    print("      ‚úÖ CPU UMAP")
                
                reduction_results['UMAP'] = {
                    'data': X_umap,
                    'model': umap_model
                }
            except Exception as e:
                print(f"      ‚ö†Ô∏è  Erreur UMAP : {e}")
        
        # === PCA ===
        print("   üìä PCA...")
        try:
            if GPU_AVAILABLE:
                pca_model = cuPCA(n_components=2, random_state=42)
                X_pca = pca_model.fit_transform(X)
                if hasattr(X_pca, 'to_pandas'):
                    X_pca = X_pca.to_pandas().values
                variance_explained = float(pca_model.explained_variance_ratio_.sum())
                print("      ‚úÖ GPU PCA")
            else:
                pca_model = PCA(n_components=2, random_state=42)
                X_pca = pca_model.fit_transform(X)
                variance_explained = float(pca_model.explained_variance_ratio_.sum())
                print("      ‚úÖ CPU PCA")
            
            reduction_results['PCA'] = {
                'data': X_pca,
                'model': pca_model,
                'variance_explained': variance_explained
            }
            
            print(f"      Variance expliqu√©e : {variance_explained:.1%}")
            
        except Exception as e:
            print(f"      ‚ö†Ô∏è  Erreur PCA : {e}")
        
        return reduction_results
    
    def business_analysis(self, df_enhanced, best_labels, clustering_method):
        """Analyse m√©tier d√©taill√©e des clusters"""
        print(f"üíº Analyse m√©tier - {clustering_method}...")
        
        df_analysis = df_enhanced.copy()
        df_analysis['cluster'] = best_labels
        
        # Statistiques par cluster
        cluster_stats = df_analysis.groupby('cluster').agg({
            'AGE': 'mean',
            'LIMIT_BAL': 'mean',
            'credit_utilization': 'mean',
            'avg_delay': 'mean',
            'avg_bill': 'mean',
            'avg_payment': 'mean',
            'risk_score': 'mean'
        }).round(2)
        
        # Tailles des clusters
        cluster_sizes = df_analysis['cluster'].value_counts().sort_index()
        
        print(f"\nüìä PROFILS DES CLUSTERS ({clustering_method}):")
        print("=" * 60)
        
        # Interpr√©tation business
        business_profiles = {}
        
        for cluster in sorted(df_analysis['cluster'].unique()):
            if cluster == -1:  # Outliers DBSCAN
                continue
                
            cluster_data = df_analysis[df_analysis['cluster'] == cluster]
            size = len(cluster_data)
            pct = size / len(df_analysis) * 100
            
            avg_age = cluster_data['AGE'].mean()
            avg_limit = cluster_data['LIMIT_BAL'].mean()
            avg_utilization = cluster_data['credit_utilization'].mean()
            avg_delay = cluster_data['avg_delay'].mean()
            avg_risk = cluster_data['risk_score'].mean()
            
            # Classification du profil
            if avg_utilization > 0.8 and avg_delay > 1:
                profile_type = "üî¥ HAUT RISQUE"
            elif avg_limit > 200000 and avg_utilization < 0.5:
                profile_type = "üíé PREMIUM"
            elif avg_utilization > 0.6:
                profile_type = "üü° RISQUE MOD√âR√â"
            else:
                profile_type = "üü¢ STANDARD"
            
            business_profiles[cluster] = {
                'type': profile_type,
                'size': size,
                'percentage': pct,
                'avg_age': avg_age,
                'avg_limit': avg_limit,
                'avg_utilization': avg_utilization,
                'avg_delay': avg_delay,
                'avg_risk': avg_risk
            }
            
            print(f"\nüè∑Ô∏è  Cluster {cluster} - {profile_type}")
            print(f"   üë• Taille : {size:,} clients ({pct:.1f}%)")
            print(f"   üë§ √Çge moyen : {avg_age:.1f} ans")
            print(f"   üí≥ Limite moyenne : {avg_limit:,.0f} NT$")
            print(f"   üìä Utilisation : {avg_utilization:.1%}")
            print(f"   ‚è∞ Retard moyen : {avg_delay:.2f}")
            print(f"   ‚ö†Ô∏è  Score risque : {avg_risk:.1f}/4")
        
        return business_profiles, cluster_stats
    
    def validate_with_target(self, best_labels):
        """Validation avec la variable cible"""
        if self.target is None:
            print("‚ö†Ô∏è  Pas de variable cible pour validation")
            return None
        
        print("\n‚úÖ VALIDATION AVEC VARIABLE CIBLE")
        print("=" * 40)
        
        df_validation = pd.DataFrame({
            'cluster': best_labels,
            'default': self.target
        })
        
        # Taux de d√©faut par cluster
        default_rates = df_validation.groupby('cluster')['default'].agg(['mean', 'count'])
        default_rates.columns = ['taux_defaut', 'nb_clients']
        default_rates['taux_defaut_pct'] = default_rates['taux_defaut'] * 100
        
        print("Taux de d√©faut par cluster :")
        for cluster, row in default_rates.iterrows():
            if cluster != -1:  # Ignore outliers
                print(f"   Cluster {cluster}: {row['taux_defaut_pct']:.1f}% ({row['nb_clients']} clients)")
        
        # Test statistique
        contingency_table = pd.crosstab(df_validation['cluster'], df_validation['default'])
        chi2, p_value, dof, expected = chi2_contingency(contingency_table)
        
        print(f"\nTest Chi-2 d'ind√©pendance :")
        print(f"   Chi-2 = {chi2:.2f}, p-value = {p_value:.2e}")
        print(f"   Diff√©rence significative : {'OUI' if p_value < 0.05 else 'NON'}")
        
        return default_rates
    
    def create_visualizations(self, X, clustering_results, reduction_results, best_method):
        """Cr√©ation des visualisations"""
        print("üìä Cr√©ation des visualisations...")
        
        fig = plt.figure(figsize=(20, 15))
        
        # === 1. COMPARAISON DES M√âTRIQUES ===
        ax1 = plt.subplot(3, 3, 1)
        
        methods = []
        silhouette_scores = []
        
        for method, result in clustering_results.items():
            if 'silhouette' in result:
                methods.append(method)
                silhouette_scores.append(result['silhouette'])
        
        bars = ax1.bar(methods, silhouette_scores, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'][:len(methods)])
        ax1.set_title('Comparaison Silhouette Score', fontsize=14, fontweight='bold')
        ax1.set_ylabel('Silhouette Score')
        ax1.tick_params(axis='x', rotation=45)
        
        # Ajout des valeurs sur les barres
        for bar, score in zip(bars, silhouette_scores):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,
                    f'{score:.3f}', ha='center', va='bottom', fontweight='bold')
        
        # === 2. VISUALISATION 2D ===
        if 'UMAP' in reduction_results:
            X_2d = reduction_results['UMAP']['data']
            title_2d = 'Clusters - UMAP 2D'
        elif 'PCA' in reduction_results:
            X_2d = reduction_results['PCA']['data']
            variance = reduction_results['PCA']['variance_explained']
            title_2d = f'Clusters - PCA 2D (Variance: {variance:.1%})'
        else:
            # Fallback PCA
            pca = PCA(n_components=2, random_state=42)
            X_2d = pca.fit_transform(X)
            title_2d = f'Clusters - PCA 2D (Variance: {pca.explained_variance_ratio_.sum():.1%})'
        
        ax2 = plt.subplot(3, 3, 2)
        best_labels = clustering_results[best_method]['labels']
        
        scatter = ax2.scatter(X_2d[:, 0], X_2d[:, 1], c=best_labels, cmap='viridis', alpha=0.6, s=1)
        ax2.set_title(f'{title_2d} - {best_method}', fontsize=14, fontweight='bold')
        ax2.set_xlabel('Composante 1')
        ax2.set_ylabel('Composante 2')
        plt.colorbar(scatter, ax=ax2)
        
        # === 3. DISTRIBUTION DES CLUSTERS ===
        ax3 = plt.subplot(3, 3, 3)
        
        cluster_counts = pd.Series(best_labels).value_counts().sort_index()
        if -1 in cluster_counts.index:
            # S√©parer outliers pour DBSCAN
            outliers = cluster_counts.pop(-1)
            colors = ['#1f77b4'] * len(cluster_counts) + ['#d62728']
            labels = [f'Cluster {i}' for i in cluster_counts.index] + [f'Outliers ({outliers})']
            sizes = list(cluster_counts.values) + [outliers]
        else:
            colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b'][:len(cluster_counts)]
            labels = [f'Cluster {i}' for i in cluster_counts.index]
            sizes = cluster_counts.values
        
        ax3.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
        ax3.set_title(f'Distribution des Clusters - {best_method}', fontsize=14, fontweight='bold')
        
        # === 4-9. PROFILS DES VARIABLES CL√âS ===
        key_features = ['AGE', 'LIMIT_BAL', 'credit_utilization', 'avg_delay', 'avg_bill', 'risk_score']
        
        for i, feature in enumerate(key_features):
            ax = plt.subplot(3, 3, 4 + i)
            
            if feature in self.df_enhanced.columns:
                for cluster in sorted(set(best_labels)):
                    if cluster != -1:  # Ignore outliers pour clart√©
                        cluster_data = self.df_enhanced.loc[best_labels == cluster, feature]
                        ax.hist(cluster_data, alpha=0.6, label=f'Cluster {cluster}', bins=20)
                
                ax.set_title(f'Distribution - {feature}', fontsize=12, fontweight='bold')
                ax.set_xlabel(feature)
                ax.set_ylabel('Fr√©quence')
                ax.legend()
            else:
                ax.text(0.5, 0.5, f'{feature}\nnon disponible', ha='center', va='center', transform=ax.transAxes)
                ax.set_title(f'{feature} - Non disponible', fontsize=12)
        
        plt.tight_layout()
        plt.savefig('clustering_results_advanced.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("‚úÖ Visualisations sauvegard√©es : clustering_results_advanced.png")
    
    def run_complete_analysis(self):
        """Ex√©cution de l'analyse compl√®te"""
        print("üöÄ D√âMARRAGE DE L'ANALYSE COMPL√àTE")
        print("=" * 60)
        
        total_start_time = time.time()
        
        # 1. Chargement des donn√©es
        self.load_data()
        
        # 2. Feature engineering
        start_time = time.time()
        self.df_enhanced = self.advanced_feature_engineering(self.df_original)
        fe_time = time.time() - start_time
        print(f"‚è±Ô∏è  Feature engineering : {fe_time:.1f}s")
        
        # 3. Preprocessing
        start_time = time.time()
        X_scaled, scaler = self.intelligent_preprocessing(self.df_enhanced)
        preprocessing_time = time.time() - start_time
        print(f"‚è±Ô∏è  Preprocessing : {preprocessing_time:.1f}s")
        
        # 4. Optimisation K
        start_time = time.time()
        optimal_k, k_metrics = self.optimize_k_parallel(X_scaled)
        k_opt_time = time.time() - start_time
        print(f"‚è±Ô∏è  Optimisation K : {k_opt_time:.1f}s")
        
        # 5. Clustering multiple
        start_time = time.time()
        clustering_results = self.advanced_clustering(X_scaled, optimal_k)
        clustering_time = time.time() - start_time
        print(f"‚è±Ô∏è  Clustering : {clustering_time:.1f}s")
        
        # 6. R√©duction dimensionnelle
        start_time = time.time()
        reduction_results = self.dimensionality_reduction(X_scaled)
        reduction_time = time.time() - start_time
        print(f"‚è±Ô∏è  R√©duction dimensionnelle : {reduction_time:.1f}s")
        
        # 7. S√©lection du meilleur algorithme
        best_method = max(clustering_results.items(), 
                         key=lambda x: x[1]['silhouette'] if 'silhouette' in x[1] else 0)[0]
        best_labels = clustering_results[best_method]['labels']
        
        print(f"\nüèÜ MEILLEUR ALGORITHME : {best_method}")
        print(f"   üìä Silhouette Score : {clustering_results[best_method]['silhouette']:.3f}")
        print(f"   üéØ Nombre de clusters : {len(set(best_labels)) - (1 if -1 in best_labels else 0)}")
        
        # 8. Analyse m√©tier
        business_profiles, cluster_stats = self.business_analysis(self.df_enhanced, best_labels, best_method)
        
        # 9. Validation
        validation_results = self.validate_with_target(best_labels)
        
        # 10. Visualisations
        self.create_visualizations(X_scaled, clustering_results, reduction_results, best_method)
        
        # 11. R√©sum√© final
        total_time = time.time() - total_start_time
        
        print(f"\nüéØ ANALYSE TERMIN√âE")
        print("=" * 40)
        print(f"‚è±Ô∏è  Temps total : {total_time:.1f}s")
        print(f"üèÜ Meilleur algorithme : {best_method}")
        print(f"üìä Silhouette Score : {clustering_results[best_method]['silhouette']:.3f}")
        print(f"üéØ Clusters identifi√©s : {len(set(best_labels)) - (1 if -1 in best_labels else 0)}")
        
        if UMAP_AVAILABLE:
            print(f"‚úÖ UMAP utilis√© pour r√©duction dimensionnelle")
        if GPU_AVAILABLE:
            print(f"‚úÖ GPU cuML utilis√© pour acc√©l√©ration")
        
        print(f"üíª Parall√©lisation sur {self.n_jobs} c≈ìurs")
        
        # Sauvegarde des r√©sultats
        self.results = {
            'best_method': best_method,
            'best_labels': best_labels,
            'clustering_results': clustering_results,
            'business_profiles': business_profiles,
            'validation_results': validation_results,
            'optimal_k': optimal_k,
            'processing_times': {
                'feature_engineering': fe_time,
                'preprocessing': preprocessing_time,
                'k_optimization': k_opt_time,
                'clustering': clustering_time,
                'reduction': reduction_time,
                'total': total_time
            }
        }
        
        return self.results

def main():
    """Fonction principale"""
    print("üíª Initialisation du clustering avanc√©...")
    
    # Initialisation avec tous les c≈ìurs disponibles
    clustering = AdvancedCreditClustering(n_jobs=-1)
    
    # Ex√©cution de l'analyse compl√®te
    results = clustering.run_complete_analysis()
    
    print("\nüéâ ANALYSE TERMIN√âE AVEC SUCC√àS!")
    return results

if __name__ == "__main__":
    results = main() 
