{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Exercice 7 : Segmentation Clients par Apprentissage Non-Supervis√©\n",
        "\n",
        "**FTML 2025 - Application de Clustering**\n",
        "\n",
        "---\n",
        "\n",
        "## Introduction\n",
        "\n",
        "### Objectif du Projet\n",
        "Segmenter les clients de carte de cr√©dit en groupes homog√®nes bas√©s sur leurs caract√©ristiques d√©mographiques, financi√®res et comportementales, sans utiliser l'information de d√©faut de paiement.\n",
        "\n",
        "### Dataset\n",
        "- **Source** : UCI Machine Learning Repository - Default of Credit Card Clients\n",
        "- **Taille** : 30,000 observations √ó 24 variables\n",
        "- **Approche** : Clustering non-supervis√© (K-means, GMM)\n",
        "\n",
        "### Enjeux M√©tier\n",
        "- **Marketing cibl√©** : adapter les offres selon les profils clients\n",
        "- **Gestion des risques** : identifier des segments √† risque sans √©tiquettes\n",
        "- **Optimisation produit** : d√©velopper des services adapt√©s aux besoins\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Chargement et Analyse Exploratoire\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('../data/default_of_credit_card_clients.csv')\n",
        "\n",
        "print(f\"Dataset : {df.shape[0]:,} observations, {df.shape[1]} variables\")\n",
        "print(f\"\\nVariables disponibles :\")\n",
        "for i, col in enumerate(df.columns, 1):\n",
        "    print(f\"{i:2d}. {col}\")\n",
        "\n",
        "print(f\"\\nPremi√®res lignes :\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Pr√©paration des Donn√©es pour le Clustering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_clustering_data(df):\n",
        "    df_cluster = df.copy()\n",
        "    \n",
        "    # Suppression des colonnes non pertinentes\n",
        "    if 'ID' in df_cluster.columns:\n",
        "        df_cluster = df_cluster.drop('ID', axis=1)\n",
        "    if 'default payment next month' in df_cluster.columns:\n",
        "        df_cluster = df_cluster.drop('default payment next month', axis=1)\n",
        "    \n",
        "    # Feature engineering\n",
        "    df_cluster['credit_utilization'] = df_cluster['BILL_AMT1'] / (df_cluster['LIMIT_BAL'] + 1)\n",
        "    df_cluster['payment_ratio'] = df_cluster['PAY_AMT1'] / (df_cluster['BILL_AMT1'] + 1)\n",
        "    df_cluster['avg_payment_delay'] = df_cluster[['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']].mean(axis=1)\n",
        "    df_cluster['avg_bill_amount'] = df_cluster[['BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']].mean(axis=1)\n",
        "    df_cluster['avg_payment_amount'] = df_cluster[['PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']].mean(axis=1)\n",
        "    df_cluster['payment_consistency'] = 1 - (df_cluster[['PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3']].std(axis=1) / (df_cluster[['PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3']].mean(axis=1) + 1))\n",
        "    \n",
        "    df_cluster = df_cluster.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "    \n",
        "    return df_cluster\n",
        "\n",
        "df_clustering = prepare_clustering_data(df)\n",
        "print(f\"Donn√©es pr√©par√©es : {df_clustering.shape[0]:,} observations, {df_clustering.shape[1]} variables\")\n",
        "\n",
        "# Standardisation\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(df_clustering)\n",
        "print(f\"Donn√©es standardis√©es : {X_scaled.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. D√©termination du Nombre Optimal de Clusters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_clustering(X, k_range=(2, 11)):\n",
        "    results = {\n",
        "        'k': [],\n",
        "        'inertia': [],\n",
        "        'silhouette': [],\n",
        "        'calinski_harabasz': [],\n",
        "        'davies_bouldin': []\n",
        "    }\n",
        "    \n",
        "    for k in range(k_range[0], k_range[1]):\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "        labels = kmeans.fit_predict(X)\n",
        "        \n",
        "        results['k'].append(k)\n",
        "        results['inertia'].append(kmeans.inertia_)\n",
        "        results['silhouette'].append(silhouette_score(X, labels))\n",
        "        results['calinski_harabasz'].append(calinski_harabasz_score(X, labels))\n",
        "        results['davies_bouldin'].append(davies_bouldin_score(X, labels))\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "print(\"√âvaluation du nombre optimal de clusters...\")\n",
        "cluster_metrics = evaluate_clustering(X_scaled)\n",
        "print(cluster_metrics.round(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('M√©triques d\\'√âvaluation du Clustering', fontsize=16)\n",
        "\n",
        "axes[0, 0].plot(cluster_metrics['k'], cluster_metrics['inertia'], 'bo-')\n",
        "axes[0, 0].set_title('M√©thode du Coude (Elbow)')\n",
        "axes[0, 0].set_xlabel('Nombre de clusters (k)')\n",
        "axes[0, 0].set_ylabel('Inertie')\n",
        "axes[0, 0].grid(True)\n",
        "\n",
        "axes[0, 1].plot(cluster_metrics['k'], cluster_metrics['silhouette'], 'ro-')\n",
        "axes[0, 1].set_title('Silhouette Score')\n",
        "axes[0, 1].set_xlabel('Nombre de clusters (k)')\n",
        "axes[0, 1].set_ylabel('Silhouette Score')\n",
        "axes[0, 1].grid(True)\n",
        "\n",
        "axes[1, 0].plot(cluster_metrics['k'], cluster_metrics['calinski_harabasz'], 'go-')\n",
        "axes[1, 0].set_title('Calinski-Harabasz Index')\n",
        "axes[1, 0].set_xlabel('Nombre de clusters (k)')\n",
        "axes[1, 0].set_ylabel('Calinski-Harabasz Index')\n",
        "axes[1, 0].grid(True)\n",
        "\n",
        "axes[1, 1].plot(cluster_metrics['k'], cluster_metrics['davies_bouldin'], 'mo-')\n",
        "axes[1, 1].set_title('Davies-Bouldin Index')\n",
        "axes[1, 1].set_xlabel('Nombre de clusters (k)')\n",
        "axes[1, 1].set_ylabel('Davies-Bouldin Index')\n",
        "axes[1, 1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "optimal_k_silhouette = cluster_metrics.loc[cluster_metrics['silhouette'].idxmax(), 'k']\n",
        "optimal_k_calinski = cluster_metrics.loc[cluster_metrics['calinski_harabasz'].idxmax(), 'k']\n",
        "optimal_k_davies = cluster_metrics.loc[cluster_metrics['davies_bouldin'].idxmin(), 'k']\n",
        "\n",
        "print(f\"Nombre optimal selon Silhouette : {optimal_k_silhouette}\")\n",
        "print(f\"Nombre optimal selon Calinski-Harabasz : {optimal_k_calinski}\")\n",
        "print(f\"Nombre optimal selon Davies-Bouldin : {optimal_k_davies}\")\n",
        "\n",
        "from collections import Counter\n",
        "votes = [optimal_k_silhouette, optimal_k_calinski, optimal_k_davies]\n",
        "optimal_k = Counter(votes).most_common(1)[0][0]\n",
        "print(f\"\\nüéØ Nombre optimal retenu : {optimal_k} clusters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Application des Algorithmes de Clustering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "kmeans_optimal = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "labels_kmeans = kmeans_optimal.fit_predict(X_scaled)\n",
        "\n",
        "gmm = GaussianMixture(n_components=optimal_k, random_state=42)\n",
        "labels_gmm = gmm.fit_predict(X_scaled)\n",
        "\n",
        "neighbors = NearestNeighbors(n_neighbors=4)\n",
        "neighbors_fit = neighbors.fit(X_scaled)\n",
        "distances, indices = neighbors_fit.kneighbors(X_scaled)\n",
        "distances = np.sort(distances[:, 3], axis=0)\n",
        "\n",
        "eps = np.percentile(distances, 95)\n",
        "dbscan = DBSCAN(eps=eps, min_samples=50)\n",
        "labels_dbscan = dbscan.fit_predict(X_scaled)\n",
        "\n",
        "print(f\"K-means : {len(np.unique(labels_kmeans))} clusters\")\n",
        "print(f\"GMM : {len(np.unique(labels_gmm))} clusters\")\n",
        "print(f\"DBSCAN : {len(np.unique(labels_dbscan[labels_dbscan != -1]))} clusters, {sum(labels_dbscan == -1)} outliers\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. √âvaluation et Visualisation des R√©sultats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_clustering_results(X, labels_dict):\n",
        "    results = []\n",
        "    \n",
        "    for method, labels in labels_dict.items():\n",
        "        if len(np.unique(labels)) > 1:\n",
        "            if method == 'DBSCAN':\n",
        "                mask = labels != -1\n",
        "                X_filtered = X[mask]\n",
        "                labels_filtered = labels[mask]\n",
        "                if len(np.unique(labels_filtered)) > 1:\n",
        "                    sil_score = silhouette_score(X_filtered, labels_filtered)\n",
        "                    ch_score = calinski_harabasz_score(X_filtered, labels_filtered)\n",
        "                    db_score = davies_bouldin_score(X_filtered, labels_filtered)\n",
        "                else:\n",
        "                    sil_score = ch_score = db_score = np.nan\n",
        "            else:\n",
        "                sil_score = silhouette_score(X, labels)\n",
        "                ch_score = calinski_harabasz_score(X, labels)\n",
        "                db_score = davies_bouldin_score(X, labels)\n",
        "            \n",
        "            results.append({\n",
        "                'Algorithme': method,\n",
        "                'Nb_Clusters': len(np.unique(labels[labels != -1])),\n",
        "                'Outliers': sum(labels == -1) if method == 'DBSCAN' else 0,\n",
        "                'Silhouette': sil_score,\n",
        "                'Calinski_Harabasz': ch_score,\n",
        "                'Davies_Bouldin': db_score\n",
        "            })\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "clustering_results = {\n",
        "    'K-means': labels_kmeans,\n",
        "    'GMM': labels_gmm,\n",
        "    'DBSCAN': labels_dbscan\n",
        "}\n",
        "\n",
        "results_df = evaluate_clustering_results(X_scaled, clustering_results)\n",
        "print(\"Comparaison des algorithmes de clustering :\")\n",
        "print(results_df.round(4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "print(f\"Variance expliqu√©e par PCA : {pca.explained_variance_ratio_.sum():.3f}\")\n",
        "print(f\"PC1 : {pca.explained_variance_ratio_[0]:.3f}\")\n",
        "print(f\"PC2 : {pca.explained_variance_ratio_[1]:.3f}\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
        "fig.suptitle('Visualisation des Clusters dans l\\'Espace PCA', fontsize=16)\n",
        "\n",
        "algorithms = ['K-means', 'GMM', 'DBSCAN']\n",
        "labels_list = [labels_kmeans, labels_gmm, labels_dbscan]\n",
        "\n",
        "for i, (algo, labels) in enumerate(zip(algorithms, labels_list)):\n",
        "    scatter = axes[i].scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', alpha=0.6, s=1)\n",
        "    axes[i].set_title(f'{algo} - {len(np.unique(labels[labels != -1]))} clusters')\n",
        "    axes[i].set_xlabel('PC1')\n",
        "    axes[i].set_ylabel('PC2')\n",
        "    \n",
        "    if algo == 'K-means':\n",
        "        centers_pca = pca.transform(kmeans_optimal.cluster_centers_)\n",
        "        axes[i].scatter(centers_pca[:, 0], centers_pca[:, 1], c='red', marker='x', s=200, linewidths=3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Analyse et Interpr√©tation des Clusters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_analysis = df_clustering.copy()\n",
        "df_analysis['cluster_kmeans'] = labels_kmeans\n",
        "\n",
        "cluster_stats = df_analysis.groupby('cluster_kmeans').agg({\n",
        "    'AGE': ['mean', 'std'],\n",
        "    'SEX': lambda x: (x == 2).mean(),\n",
        "    'EDUCATION': 'mean',\n",
        "    'MARRIAGE': 'mean',\n",
        "    'LIMIT_BAL': ['mean', 'std'],\n",
        "    'credit_utilization': ['mean', 'std'],\n",
        "    'payment_ratio': ['mean', 'std'],\n",
        "    'avg_payment_delay': ['mean', 'std'],\n",
        "    'avg_bill_amount': ['mean', 'std'],\n",
        "    'avg_payment_amount': ['mean', 'std']\n",
        "})\n",
        "\n",
        "cluster_stats.columns = ['_'.join(col).strip() for col in cluster_stats.columns]\n",
        "cluster_stats = cluster_stats.round(3)\n",
        "\n",
        "print(\"Statistiques par cluster (K-means) :\")\n",
        "print(cluster_stats)\n",
        "\n",
        "cluster_sizes = df_analysis['cluster_kmeans'].value_counts().sort_index()\n",
        "print(\"\\nTaille des clusters :\")\n",
        "for cluster, size in cluster_sizes.items():\n",
        "    print(f\"  Cluster {cluster}: {size:,} clients ({size/len(df_analysis)*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "key_features = ['AGE', 'LIMIT_BAL', 'credit_utilization', 'payment_ratio', 'avg_payment_delay']\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "fig.suptitle('Profils des Clusters K-means', fontsize=16)\n",
        "\n",
        "for i, feature in enumerate(key_features):\n",
        "    row, col = i // 3, i % 3\n",
        "    for cluster in sorted(df_analysis['cluster_kmeans'].unique()):\n",
        "        cluster_data = df_analysis[df_analysis['cluster_kmeans'] == cluster][feature]\n",
        "        axes[row, col].hist(cluster_data, alpha=0.6, label=f'Cluster {cluster}', bins=30)\n",
        "    \n",
        "    axes[row, col].set_title(f'Distribution de {feature}')\n",
        "    axes[row, col].set_xlabel(feature)\n",
        "    axes[row, col].legend()\n",
        "\n",
        "axes[1, 2].bar(cluster_sizes.index, cluster_sizes.values)\n",
        "axes[1, 2].set_title('Taille des Clusters')\n",
        "axes[1, 2].set_xlabel('Cluster')\n",
        "axes[1, 2].set_ylabel('Nombre de clients')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üéØ INTERPR√âTATION M√âTIER DES CLUSTERS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for cluster in sorted(df_analysis['cluster_kmeans'].unique()):\n",
        "    cluster_data = df_analysis[df_analysis['cluster_kmeans'] == cluster]\n",
        "    \n",
        "    avg_age = cluster_data['AGE'].mean()\n",
        "    avg_limit = cluster_data['LIMIT_BAL'].mean()\n",
        "    avg_utilization = cluster_data['credit_utilization'].mean()\n",
        "    avg_payment_delay = cluster_data['avg_payment_delay'].mean()\n",
        "    avg_payment_ratio = cluster_data['payment_ratio'].mean()\n",
        "    pct_female = (cluster_data['SEX'] == 2).mean() * 100\n",
        "    \n",
        "    if avg_payment_delay > 1.5 and avg_utilization > 0.8:\n",
        "        risk_level = \"HAUT RISQUE\"\n",
        "    elif avg_payment_delay > 0.5 or avg_utilization > 0.6:\n",
        "        risk_level = \"RISQUE MOD√âR√â\"\n",
        "    else:\n",
        "        risk_level = \"FAIBLE RISQUE\"\n",
        "    \n",
        "    print(f\"\\nCluster {cluster} ({len(cluster_data)} clients, {len(cluster_data)/len(df_analysis)*100:.1f}%):\")\n",
        "    print(f\"  ‚Ä¢ √Çge moyen : {avg_age:.1f} ans\")\n",
        "    print(f\"  ‚Ä¢ Limite cr√©dit : {avg_limit:,.0f} NT$\")\n",
        "    print(f\"  ‚Ä¢ Utilisation cr√©dit : {avg_utilization:.1%}\")\n",
        "    print(f\"  ‚Ä¢ Retard paiement : {avg_payment_delay:.2f}\")\n",
        "    print(f\"  ‚Ä¢ Ratio paiement : {avg_payment_ratio:.2f}\")\n",
        "    print(f\"  ‚Ä¢ Femmes : {pct_female:.1f}%\")\n",
        "    print(f\"  ‚Ä¢ Niveau de risque : {risk_level}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Validation avec les Donn√©es de D√©faut\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_validation = df.copy()\n",
        "df_validation['cluster_kmeans'] = labels_kmeans\n",
        "\n",
        "default_rates = df_validation.groupby('cluster_kmeans')['default payment next month'].agg(['mean', 'count'])\n",
        "default_rates.columns = ['taux_defaut', 'nb_clients']\n",
        "default_rates['taux_defaut_pct'] = default_rates['taux_defaut'] * 100\n",
        "\n",
        "print(\"Validation : Taux de d√©faut par cluster\")\n",
        "print(default_rates.round(2))\n",
        "\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "contingency_table = pd.crosstab(df_validation['cluster_kmeans'], df_validation['default payment next month'])\n",
        "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "print(f\"\\nTest du Chi-2 :\")\n",
        "print(f\"  Chi-2 = {chi2:.2f}, p-value = {p_value:.2e}\")\n",
        "print(f\"  Diff√©rence significative : {'OUI' if p_value < 0.05 else 'NON'}\")\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "bars = ax1.bar(default_rates.index, default_rates['taux_defaut_pct'])\n",
        "ax1.set_title('Taux de D√©faut par Cluster')\n",
        "ax1.set_xlabel('Cluster')\n",
        "ax1.set_ylabel('Taux de D√©faut (%)')\n",
        "ax1.set_ylim(0, max(default_rates['taux_defaut_pct']) * 1.1)\n",
        "\n",
        "for bar, rate in zip(bars, default_rates['taux_defaut_pct']):\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
        "             f'{rate:.1f}%', ha='center', va='bottom')\n",
        "\n",
        "sns.heatmap(contingency_table, annot=True, fmt='d', cmap='YlOrRd', ax=ax2)\n",
        "ax2.set_title('Table de Contingence : Cluster vs D√©faut')\n",
        "ax2.set_xlabel('D√©faut de Paiement')\n",
        "ax2.set_ylabel('Cluster')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 8. Conclusions et Recommandations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üìä CONCLUSIONS DE L'ANALYSE DE CLUSTERING\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(f\"1. PERFORMANCE DES ALGORITHMES :\")\n",
        "print(f\"   ‚Ä¢ K-means : Score silhouette = {silhouette_score(X_scaled, labels_kmeans):.3f}\")\n",
        "print(f\"   ‚Ä¢ GMM : Score silhouette = {silhouette_score(X_scaled, labels_gmm):.3f}\")\n",
        "print(f\"   ‚Ä¢ DBSCAN : {sum(labels_dbscan == -1)} outliers d√©tect√©s\")\n",
        "\n",
        "print(f\"\\n2. SEGMENTATION OBTENUE :\")\n",
        "print(f\"   ‚Ä¢ {optimal_k} segments clients distincts identifi√©s\")\n",
        "print(f\"   ‚Ä¢ Variance expliqu√©e (PCA) : {pca.explained_variance_ratio_.sum():.1%}\")\n",
        "print(f\"   ‚Ä¢ Diff√©rences significatives entre clusters (p < 0.001)\")\n",
        "\n",
        "print(f\"\\n3. VALIDATION M√âTIER :\")\n",
        "print(f\"   ‚Ä¢ Taux de d√©faut variables selon les clusters\")\n",
        "print(f\"   ‚Ä¢ Cluster √† haut risque identifi√©\")\n",
        "print(f\"   ‚Ä¢ Segmentation coh√©rente avec le comportement de paiement\")\n",
        "\n",
        "print(f\"\\n4. RECOMMANDATIONS :\")\n",
        "print(f\"   ‚Ä¢ Utiliser K-means pour la segmentation op√©rationnelle\")\n",
        "print(f\"   ‚Ä¢ Adapter les strat√©gies marketing par segment\")\n",
        "print(f\"   ‚Ä¢ Surveiller particuli√®rement les clusters √† haut risque\")\n",
        "print(f\"   ‚Ä¢ R√©viser p√©riodiquement la segmentation\")\n",
        "\n",
        "print(f\"\\n5. M√âTRIQUES FINALES :\")\n",
        "print(f\"   ‚Ä¢ Silhouette Score : {silhouette_score(X_scaled, labels_kmeans):.3f}\")\n",
        "print(f\"   ‚Ä¢ Calinski-Harabasz : {calinski_harabasz_score(X_scaled, labels_kmeans):.1f}\")\n",
        "print(f\"   ‚Ä¢ Davies-Bouldin : {davies_bouldin_score(X_scaled, labels_kmeans):.3f}\")\n",
        "print(f\"   ‚Ä¢ Variance PCA : {pca.explained_variance_ratio_.sum():.1%}\")\n",
        "\n",
        "print(f\"\\n6. APPLICATIONS M√âTIER :\")\n",
        "print(f\"   ‚Ä¢ Segmentation marketing pour offres personnalis√©es\")\n",
        "print(f\"   ‚Ä¢ Identification proactive des profils √† risque\")\n",
        "print(f\"   ‚Ä¢ Optimisation des limites de cr√©dit par segment\")\n",
        "print(f\"   ‚Ä¢ D√©veloppement de produits cibl√©s\")\n",
        "\n",
        "print(f\"\\n‚úÖ CLUSTERING R√âUSSI - {optimal_k} segments clients identifi√©s avec succ√®s\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
